```markdown
# Multimodal Chatbot API Documentation

This API allows you to interact with a multimodal chatbot that can process text queries,  retrieve relevant information (including images) from a pre-loaded knowledge base, and generate responses using a Large Language Model (LLM).  It leverages FAISS for efficient similarity search, Sentence Transformers for text embeddings, and Google's Gemini for multimodal response generation.

## Endpoints

### `/chat` (POST)

This is the primary endpoint for interacting with the chatbot.

**Request Body (JSON):**

```json
{
  "query": "Your question here",
  "history": [  // Optional:  Array of previous conversation turns (see below)
    {"role": "user", "parts": ["Previous user message"]},
    {"role": "model", "parts": ["Previous model response"]}
  ],
  "top_k": 5  // Optional: Number of top results to retrieve from the knowledge base (default: 5)
}
```

*   **`query`** (string, required):  The user's question or statement.
*   **`history`** (array, optional):  An array representing the conversation history.  Each element in the array is a dictionary with `role` ("user" or "model") and `parts` (an array of strings, representing the content of the message).  This is crucial for maintaining context in multi-turn conversations.  If omitted, the chatbot treats the current query as a new conversation.
*   **`top_k`** (integer, optional):  The number of most relevant documents to retrieve from the FAISS index.  These documents form the context for the LLM. Defaults to 5.

**Response Body (JSON):**

```json
{
  "text": "The chatbot's response to your query.",
  "images": [
    "https://storage.googleapis.com/llmimages/data%5Cpath%5Cto%5Cimage1.jpg",
    "https://storage.googleapis.com/llmimages/data%5Cpath%5Cto%5Cimage2.png"
  ],
  "summaries": [
    "Summary of document 1 related to the query.",
    "Summary of document 2 related to the query.",
    "..."
  ]
}
```

*   **`text`** (string): The textual response generated by the LLM, taking into account the user's query, context from the knowledge base, and potentially relevant images.
*   **`images`** (array of strings):  An array of *absolute* URLs to images that the LLM deemed relevant to the response. These URLs point to images stored on Google Cloud Storage.  This array may be empty if the LLM did not find any relevant images, even if images were present in the retrieved documents.  The LLM is instructed to select 0-3 images.
*   **`summaries`** (array of strings):  An array of summaries extracted from the documents retrieved from the FAISS index (the top *k* documents). These provide additional context and source information.

**Example Request (using `fetch` in JavaScript):**

```javascript
const query = "What are the key features of the product shown in image XYZ?";
const history = [
    { role: "user", parts: ["Show me some products."] },
    { role: "model", parts: ["Here are some products: ..."] }
];
const top_k = 7;

fetch('http://your-server-address:5000/chat', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({ query, history, top_k })
})
.then(response => response.json())
.then(data => {
  console.log('Response:', data);
  // Display the text response:
  document.getElementById('response-text').innerText = data.text;

  // Display the images (if any):
  const imageContainer = document.getElementById('image-container');
  imageContainer.innerHTML = ''; // Clear previous images
  data.images.forEach(imageUrl => {
    const img = document.createElement('img');
    img.src = imageUrl;
    img.alt = "Relevant Image";
    img.style.maxWidth = "200px"; // Optional: Limit image size
    img.style.marginRight = "10px";
    imageContainer.appendChild(img);
  });

  // Display the summaries:
  const summariesContainer = document.getElementById('summaries-container');
    summariesContainer.innerHTML = ''; // Clear previous summaries.
  data.summaries.forEach(summary => {
      const p = document.createElement('p');
      p.textContent = summary;
      summariesContainer.appendChild(p);
  });
})
.catch(error => console.error('Error:', error));

```

**Example HTML Structure (for the JavaScript example):**

```html
<div>
  <p id="response-text"></p>
  <div id="image-container"></div>
  <div id="summaries-container"></div>
</div>
```

**Error Handling:**

*   If an error occurs during processing (e.g., image download failure, LLM error, JSON parsing error), the API will return a JSON response with a `500` status code and an `error` key:

    ```json
    {
      "error": "Description of the error"
    }
    ```

### `/` (GET)

This endpoint serves a simple message indicating that the chatbot API is running and that the `/chat` endpoint should be used.

**Response Body (text):**

```
Use the /chat endpoint to interact with the chatbot.
```
## Important Considerations and Implementation Details

*   **Image URLs:** The `images` array in the response contains *absolute* URLs to images hosted on Google Cloud Storage.  You can directly use these URLs in `<img>` tags or to fetch the image data. The base URL for images is `https://storage.googleapis.com/llmimages/data%5C`.  The API converts relative paths from the original dataset to these cloud URLs.

*   **Multimodal Input:** The Gemini model used (`gemini-1.5-flash-8b`) is multimodal, meaning it can process both text and images. The API downloads relevant images (up to 15, to manage resources) based on the semantic search results and passes them to the LLM, along with the text query and context. The images is downloaded concurrently with `ThreadPoolExecutor`.

*   **LLM Response Format:** The LLM is instructed to return a JSON object with `text` and `images` keys. The code includes error handling to try different parsing methods if the initial parse fails.  The `parse_gemini_markdown_to_dict` function attempts to extract JSON from Gemini's markdown output.

*   **Concurrency and Threading:**  The API uses `index_lock` (for FAISS index access) and `gemini_lock` (for Gemini API calls) to ensure thread safety, as the Flask app is configured to be multi-threaded. The downloads are handled concurrently using.

*   **Dependencies:** The API has several dependencies, including:
    *   `faiss-cpu`: For efficient similarity search.
    *   `sentence-transformers`: For generating text embeddings.
    *   `pandas`: For data manipulation.
    *   `Flask`: For the web application framework.
    *   `google-generativeai`: For interacting with the Gemini API.
    *   `requests`: For downloading images.
    *  `Pillow (PIL)`: to handle images.
    *   `flask_cors`: to handle CORS.

* **CORS** The API supports Cross-Origin Resource Sharing (CORS) using `flask_cors`. This allows requests from different origins (e.g., your frontend running on a different port or domain).

*   **Logging:**  The API uses the `logging` module to provide information about its operation, including image downloads, errors, and warnings.  This is helpful for debugging.

This comprehensive documentation provides a clear understanding of how to use the Multimodal Chatbot API, including request/response formats, example code, and important implementation details. This helps the frontend developer to understand how to send correct requests and how to handle and display correct responses.
```
